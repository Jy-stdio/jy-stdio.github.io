<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jy-stdio.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jy-stdio.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-08T15:53:57+00:00</updated><id>https://jy-stdio.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://jy-stdio.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://jy-stdio.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post as a sidebar, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2>

<p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">acccpost with redirect</title><link href="https://jy-stdio.github.io/blog/2023/Transformer/" rel="alternate" type="text/html" title="acccpost with redirect" /><published>2023-01-22T17:39:00+00:00</published><updated>2023-01-22T17:39:00+00:00</updated><id>https://jy-stdio.github.io/blog/2023/Transformer</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2023/Transformer/"><![CDATA[<h1 id="transformer">Transformer</h1>

<p>date: May 6, 2023, author: @Jiyao Liu</p>

<h2 id="参考资料">参考资料</h2>

<p><a href="https://zhuanlan.zhihu.com/p/340149804?utm_medium=social&amp;utm_oi=998216950467743744&amp;utm_psn=1636827853579972608&amp;utm_source=wechat_session">Vision Transformer 超详细解读 (原理分析+代码解读) (一)</a></p>

<p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">https://github.com/jadore801120/attention-is-all-you-need-pytorch</a></p>

<blockquote>
  <p>Transformer 模型使用了 Self-Attention 机制，<strong>不采用</strong> RNN 的<strong>顺序结构</strong>，使得模型<strong>可以并行化训练</strong>，而且能够<strong>拥有全局信息。</strong></p>

</blockquote>

<h2 id="1-self-attention">1. Self-attention</h2>

<h3 id="11-处理sequence数据的模型"><strong>1.1 处理Sequence数据的模型</strong></h3>

<ul>
  <li>
    <p>RNN</p>

    <p>如果假设是一个single directional的RNN，那当输出$b_4$时，默认\(a_1,a_2,a_3,a_4\)都已经看过了。</p>

    <p>如果假设是一个bi-directional的RNN，那当输出任意$b_{任意}$任意时，默认$a_1,a_2,a_3,a_4$都已经看过了</p>

    <p>RNN很不容易并行化：必须逐个元素计算（左1）；CNN代替RNN实现并行计算，但是只能考虑有限内容，通过多堆叠几层CNN，使得CNN的感受野变大，如下图：
  <!--     ![Untitled](images/2023-05-08-Transformer/Untitled.png) --></p>

    <div class="row mt-3">
      <div class="col-sm mt-3 mt-md-0">
      <figure>

<picture>
    
  <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/_posts/images/2023-05-08-Transformer/Untitled-480.webp" />
  <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/_posts/images/2023-05-08-Transformer/Untitled-800.webp" />
  <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/_posts/images/2023-05-08-Transformer/Untitled-1400.webp" />
    

  <!-- Fallback to the original file -->
  <img src="/_posts/images/2023-05-08-Transformer/Untitled.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

</picture>

</figure></div></div>
  </li>
</ul>
<p>&lt;/figure&gt;</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    &lt;/div&gt;
&lt;/div&gt;
&lt;div class="caption"&gt;
    ddddd
&lt;/div&gt;
</code></pre></div></div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
	<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/9-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/9-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/9-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
	<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>使用self-attention代替，可以减少堆叠CNN filter，并且可以实现并行化计算的bi-directional seq2seq。

![截屏2023-05-07 15.04.13.png](images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_15.04.13.png)
</code></pre></div></div>

<h3 id="12-self-attention">1.2 Self-attention</h3>

<ul>
  <li>
    <p>self-attention具体是怎么做的？</p>

    <p>(3个不同的transformation matrix $W^q,W^k,W^v$)
  首先计算q,k,v：</p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_15.05.25.png" alt="截屏2023-05-07 15.05.25.png" /></p>

    <p>接着计算每个$q_j$和每个$k_i$的attention $\alpha_{i,j}$：</p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_15.07.42.png" alt="截屏2023-05-07 15.07.42.png" /></p>

    <p>然后对每个$\alpha_{j,i}$取softmax：</p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_15.13.22.png" alt="截屏2023-05-07 15.13.22.png" /></p>

    <p>最终计算attention $\alpha$和value $v$的加权和，即输出b：</p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_15.15.31.png" alt="截屏2023-05-07 15.15.31.png" /></p>
  </li>
  <li>
    <p>矩阵化self-attention计算过程</p>

    <p>首先计算q，k，v</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%201.png" alt="Untitled" /></p>

    <p>接着计算attention $\alpha$：</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%202.png" alt="Untitled" /></p>

    <p>总的计算过程可以表示为：</p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_15.49.05.png" alt="截屏2023-05-07 15.49.05.png" /></p>
  </li>
</ul>

<h3 id="13-multi-head-self-attention">1.3 Multi-head self-attention</h3>

<ul>
  <li>
    <p>multi-head self-attention</p>

    <p>a乘以不同的W得到多个QKV，通过多个QKV计算得到多个$b^{i,j}$，在乘以$W^0$得到最终的$b^i$。</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%203.png" alt="Untitled" /></p>

    <p>这里有一组Multi-head Self-attention的解果，其中绿色部分是一组query和key，红色部分是另外一组query和key，可以发现绿色部分其实更关注global的信息，而红色部分其实更关注local的信息。</p>

    <p><img src="images/2023-05-08-Transformer/%E6%88%AA%E5%B1%8F2023-05-07_16.02.45.png" alt="%E6%88%AA%E5%B1%8F2023-05-07_16.02.45" /></p>

    <p>代码实现：</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="s">''' Multi-Head Attention module '''</span>
    
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  				<span class="s">"""
  				n_head：head数；d_model：最大句子的词个数；d_k,d_v：k,v向量维度
  				"""</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    
          <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
          <span class="n">self</span><span class="p">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
          <span class="n">self</span><span class="p">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_v</span>
    
          <span class="n">self</span><span class="p">.</span><span class="n">w_qs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">w_ks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">*</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">w_vs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_head</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
          <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">d_k</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
    
          <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
          <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_v</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span>
          <span class="n">sz_b</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">,</span> <span class="n">len_v</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">v</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
          <span class="n">residual</span> <span class="o">=</span> <span class="n">q</span>
    
          <span class="c1"># Pass through the pre-attention projection: b x lq x (n*dv)
</span>          <span class="c1"># Separate different heads: b x lq x n x dv
</span>          <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_qs</span><span class="p">(</span><span class="n">q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">sz_b</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
          <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_ks</span><span class="p">(</span><span class="n">k</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">sz_b</span><span class="p">,</span> <span class="n">len_k</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
          <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_vs</span><span class="p">(</span><span class="n">v</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">sz_b</span><span class="p">,</span> <span class="n">len_v</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_v</span><span class="p">)</span>
    
          <span class="c1"># Transpose for attention dot product: b x n x lq x dv
</span>          <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">v</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
          <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># For head axis broadcasting.
</span>    
          <span class="n">q</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    
          <span class="c1">#q (sz_b,n_head,N=len_q,d_k)
</span>          <span class="c1">#k (sz_b,n_head,N=len_k,d_k)
</span>          <span class="c1">#v (sz_b,n_head,N=len_v,d_v)
</span>    
          <span class="c1"># Transpose to move the head dimension back: b x lq x n x dv
</span>          <span class="c1"># Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
</span>          <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">sz_b</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
          <span class="c1">#q (sz_b,len_q,n_head,N * d_k)
</span>          <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
          <span class="n">q</span> <span class="o">+=</span> <span class="n">residual</span>
    
          <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    
          <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">attn</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="14-positional-encoding">1.4 <strong>Positional Encoding</strong></h3>

<ul>
  <li>
    <p>原始positional encoding</p>

    <p>现在的self-attention中没有位置的信息，一个单词向量的“近在咫尺”位置的单词向量和“远在天涯”位置的单词向量效果是一样的。所以在self-attention原来的paper中，是这样解决的：人工对每个位置构建一个位置向量$p_i$，通过对其进行认为变换$W^p$，得到position embedding $e_i$:</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%204.png" alt="Untitled" /></p>

    <p>在transformer论文中，position embedding的计算公式（$W^p$）为(左)，每个$PE(i)=e_i$的计算公式如下：</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%205.png" alt="Untitled" /></p>

    <p>代码实现：</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">,</span> <span class="n">n_position</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
          <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    
          <span class="c1"># Not a parameter（下面说明了这个的作用）
</span>          <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="s">'pos_table'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_sinusoid_encoding_table</span><span class="p">(</span><span class="n">n_position</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">))</span>
    
      <span class="k">def</span> <span class="nf">_get_sinusoid_encoding_table</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_position</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">):</span>
          <span class="s">''' Sinusoid position encoding table '''</span>
          <span class="c1"># TODO: make it with torch instead of numpy
</span>  				<span class="c1"># d_hid：公式中的512，即文本的最大长度。总的来说是为了计算效率，所以把长度限制在了512。
</span>          <span class="k">def</span> <span class="nf">get_position_angle_vec</span><span class="p">(</span><span class="n">position</span><span class="p">):</span>
              <span class="k">return</span> <span class="p">[</span><span class="n">position</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">hid_j</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_hid</span><span class="p">)</span> <span class="k">for</span> <span class="n">hid_j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">d_hid</span><span class="p">)]</span>
    
          <span class="n">sinusoid_table</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">get_position_angle_vec</span><span class="p">(</span><span class="n">pos_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">pos_i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_position</span><span class="p">)])</span>
          <span class="n">sinusoid_table</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">sinusoid_table</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i
</span>          <span class="n">sinusoid_table</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">sinusoid_table</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i+1
</span>    
          <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">sinusoid_table</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="c1">#(1,N,d)
</span>    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="c1"># x(B,N,d)
</span>          <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pos_table</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)].</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
</code></pre></div>    </div>

    <blockquote>
      <p>注：<strong><code class="language-plaintext highlighter-rouge">[register_buffer()</code></strong> 是 <strong><code class="language-plaintext highlighter-rouge">nn.Module</code></strong> 类中的一个方法，它用于向模块添加一个持久缓冲区。这通常用于注册不应被视为模型参数的缓冲区。例如，BatchNorm 的 <strong><code class="language-plaintext highlighter-rouge">running_mean</code></strong> 不是参数，但它是持久状态的一部分<strong>1</strong>](https://zhuanlan.zhihu.com/p/100000785)<a href="https://zhuanlan.zhihu.com/p/464825510">。这意味着在模型训练时，该组参数不会更新（即调用 <strong><code class="language-plaintext highlighter-rouge">optimizer.step()</code></strong> 后该组参数不会变化，只能人为地改变它们的值），但在保存模型时，该组参数又作为模型参数不可或缺的一部分被保存<strong>2</strong></a>。</p>

    </blockquote>

    <p><img src="images/2023-05-08-Transformer/Untitled%206.png" alt="Untitled" /></p>
  </li>
</ul>

<h3 id="15-self-attention-总结">1.5 self-attention 总结</h3>

<ul>
  <li>
    <p>self-attention在sequence2sequence model里面是怎么使用</p>

    <p>可以把Encoder-Decoder中的RNN用self-attention取代掉。</p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_16.38.50.png" alt="截屏2023-05-07 16.38.50.png" /></p>
  </li>
  <li>
    <p>对比self-attention和CNN的关系</p>

    <p>self-attention是一种复杂化的CNN，在做CNN的时候是只考虑感受野里面的资讯。但是self-attention由attention找到相关的pixel，就好像是感受野的范围和大小是自动被学出来的，所以CNN可以看做是self-attention的特例。既然self-attention是更广义的CNN，则这个模型更加flexible。一个模型越flexible，训练它所需要的数据量就越多，所以在训练self-attention模型时就需要更多的数据，这一点在下面介绍的论文 ViT 中有印证，它需要的数据集是有3亿张图片的JFT-300，而如果不使用这么多数据而只使用ImageNet，则性能不如CNN。</p>
  </li>
</ul>

<h2 id="2-transformer实现及代码">2. Transformer实现及代码</h2>

<h3 id="21-transformer原理分析">2.1 Transformer原理分析</h3>

<ul>
  <li>
    <p>Transformer算法步骤</p>

    <p><strong>模型总览</strong></p>

    <p><img src="images/2023-05-08-Transformer/Untitled%207.png" alt="Untitled" /></p>

    <p><strong>Encoder</strong></p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_16.56.16.png" alt="截屏2023-05-07 16.56.16.png" /></p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_16.57.17.png" alt="截屏2023-05-07 16.57.17.png" /></p>

    <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_17.20.06.png" alt="截屏2023-05-07 17.20.06.png" /></p>

    <p>接着是一个Feed Forward的前馈网络和一个Add &amp; Norm Layer。代码如下：</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="s">''' A two-feed-forward-layer module '''</span>
    
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">)</span> <span class="c1"># position-wise
</span>          <span class="n">self</span><span class="p">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_hid</span><span class="p">,</span> <span class="n">d_in</span><span class="p">)</span> <span class="c1"># position-wise
</span>          <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    
          <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
    
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_2</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">w_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">+=</span> <span class="n">residual</span>
    
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div>    </div>

    <p><strong>Decoder</strong></p>

    <p>输入：输入包括2部分，第一部分是最下方是前一个time step的输出序列的 embedding $I$，再加上一个表示位置的Positional Encoding $E$，第二部分是encoder的输出。绿色的block中，首先是Masked Multi-Head Self-attention，masked的意思是使attention只会关注已经产生的输出sequence，即只保留$i-1$位置的输出序列（这里的输出序列指的是例如文本翻译得到的目标文本，而不是encoder的编码输出）。
  Masked Multi-Head Self-attention之后紧接一个多头注意力层，它的Key和Value来自encoder，Query来自上一位置Decoder的的输出。</p>

    <p>输出：最后有一个 Softmax 层计算下一个翻译单词的概率。</p>

    <p>详解Masked Multi-Head Self-attention（对照前文attention的计算过程）（左），右图为Decoder的过程，绿色框外为mask掉的区域。</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%208.png" alt="Untitled" /></p>

    <p><strong>测试</strong></p>

    <ol>
      <li>将输入通过encoder编码；</li>
      <li>输入<Begin>，解码器输出 I 。</Begin></li>
      <li>输入前面已经解码的<Begin>和 I，解码器输出have。</Begin></li>
      <li>输入已经解码的<Begin>，I, have, a, cat，解码器输出解码结束标志位<end>，每次解码都会利用前面已经解码输出的所有单词嵌入信息。</end></Begin></li>
    </ol>
  </li>
  <li>
    <p>矩阵化Masked Multi-Head Self-attention</p>

    <p><img src="images/2023-05-08-Transformer/Untitled%209.png" alt="Untitled" /></p>

    <p>代码实现：<strong>ScaledDotProductAttention</strong></p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="s">''' Scaled Dot-Product Attention '''</span>
    
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
          <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_dropout</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
          <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    
          <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
              <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    
          <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
          <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    
          <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn</span> <span class="c1"># 输出为预测结果z和attention值
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>为什么第2个<strong>Multi-Head Self-attention</strong>的Query来自第1个Self-attention layer的输出，Key, Value来自Encoder的输出？</p>

    <p>答;来自Transformer Encoder的输出，所以可以看做<strong>句子(Sequence)/图片(image)</strong>的<strong>内容信息(content，比如句意是：”我有一只猫”)。QUery</strong>表达了一种诉求：希望得到什么，可以看做<strong>引导信息(guide)</strong>。通过Multi-Head Self-attention结合在一起的过程就相当于是<strong>把我们需要的内容信息指导表达出来</strong>。</p>
  </li>
</ul>

<h3 id="22-transformer顶层代码">2.2 Transformer顶层代码</h3>

<p><strong>产生mask</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pad_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">pad_idx</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">seq</span> <span class="o">!=</span> <span class="n">pad_idx</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_subsequent_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
    <span class="s">''' For masking out the subsequent info. '''</span>
    <span class="n">sz_b</span><span class="p">,</span> <span class="n">len_s</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
    <span class="n">subsequent_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">len_s</span><span class="p">,</span> <span class="n">len_s</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">seq</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)).</span><span class="nf">bool</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">subsequent_mask</span>
</code></pre></div></div>

<blockquote>
  <p>src_mask = get_pad_mask(src_seq, self.src_pad_idx)</p>

  <p>用于产生Encoder的Mask，它是一列Bool值，负责把标点mask掉。</p>

  <p>trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) &amp; get_subsequent_mask(trg_seq)</p>

  <p>用于产生Decoder的Mask。它是一个矩阵，如图24中的Mask所示，功能已在上文介绍。</p>

</blockquote>

<p><strong>Encoder block：</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">classEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">''' Compose with two layers '''</span>

		<span class="nf">def__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
						<span class="s">"""
							n_head：head数；d_model：最大句子的词个数；d_k,d_v：k,v向量维度
						"""</span>
		        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
		        <span class="n">self</span><span class="p">.</span><span class="n">slf_attn</span><span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
		        <span class="n">self</span><span class="p">.</span><span class="n">pos_ffn</span><span class="o">=</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
		
		<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">enc_input</span><span class="p">,</span> <span class="n">slf_attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
		        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_slf_attn</span><span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">slf_attn</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_input</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">slf_attn_mask</span><span class="p">)</span>
		        <span class="n">enc_output</span><span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_ffn</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_slf_attn</span>
</code></pre></div></div>

<p><strong>Decoder block</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">''' Compose with three layers '''</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">slf_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">enc_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pos_ffn</span> <span class="o">=</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dec_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">slf_attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_slf_attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">slf_attn</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">dec_input</span><span class="p">,</span> <span class="n">dec_input</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">slf_attn_mask</span><span class="p">)</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">enc_attn</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">dec_enc_attn_mask</span><span class="p">)</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_ffn</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_slf_attn</span><span class="p">,</span> <span class="n">dec_enc_attn</span>
</code></pre></div></div>

<p><strong>Encoder:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">''' A encoder model with self attention mechanism. '''</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="n">self</span><span class="p">,</span> <span class="n">n_src_vocab</span><span class="p">,</span> <span class="n">d_word_vec</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">pad_idx</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_position</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">scale_emb</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">src_word_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_src_vocab</span><span class="p">,</span> <span class="n">d_word_vec</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">pad_idx</span><span class="p">)</span> <span class="c1"># 补充说明
</span>        <span class="n">self</span><span class="p">.</span><span class="n">position_enc</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_word_vec</span><span class="p">,</span> <span class="n">n_position</span><span class="o">=</span><span class="n">n_position</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale_emb</span> <span class="o">=</span> <span class="n">scale_emb</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_seq</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">return_attns</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="n">enc_slf_attn_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># -- Forward
</span>        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">src_word_emb</span><span class="p">(</span><span class="n">src_seq</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">scale_emb</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">*=</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">position_enc</span><span class="p">(</span><span class="n">enc_output</span><span class="p">))</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">enc_output</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">enc_layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_stack</span><span class="p">:</span>
            <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_slf_attn</span> <span class="o">=</span> <span class="nf">enc_layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">slf_attn_mask</span><span class="o">=</span><span class="n">src_mask</span><span class="p">)</span>
            <span class="n">enc_slf_attn_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">enc_slf_attn</span><span class="p">]</span> <span class="k">if</span> <span class="n">return_attns</span> <span class="k">else</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">return_attns</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_slf_attn_list</span>
        <span class="k">return</span> <span class="n">enc_output</span><span class="p">,</span>
</code></pre></div></div>

<blockquote>
  <p>说明：
1.nn.Embedding: 创建一个字典向量组成的矩阵，将句子输入转化为矩阵，例如：</p>

  <blockquote>
    <blockquote>
      <blockquote>
        <blockquote>
          <p>embedding = nn.Embedding(10, 3) # 创建一个词典，一共十个词，每个词用维度为3的向量表示。
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) # 两个句子
embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],  # 每一行代表一个词，第一个词索引为1，则这个向量就是词典中索引为1的行的向量值
[-0.6431,  0.0748,  0.6969],
[ 1.4970,  1.3448, -0.9685],
[-0.3677, -2.7265, -0.1685]],
     [[ 1.4970,  1.3448, -0.9685],
     [ 0.4362, -0.4004,  0.9400],
     [-0.6431,  0.0748,  0.6969],
     [ 0.9124, -2.3616,  1.1151]]])</p>
        </blockquote>
      </blockquote>
    </blockquote>

  </blockquote>

  <p>2.nn.LayerNorm(d_model, eps=1e-6)</p>

  <p><img src="images/2023-05-08-Transformer/%25E6%2588%25AA%25E5%25B1%258F2023-05-07_22.28.50.png" alt="截屏2023-05-07 22.28.50.png" /></p>

</blockquote>

<p><strong>Decoder</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">''' A decoder model with self attention mechanism. '''</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trg_seq</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">return_attns</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="n">dec_slf_attn_list</span><span class="p">,</span> <span class="n">dec_enc_attn_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="c1"># -- Forward
</span>        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">position_enc</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">trg_word_emb</span><span class="p">(</span><span class="n">trg_seq</span><span class="p">)))</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">dec_layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_stack</span><span class="p">:</span>
            <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_slf_attn</span><span class="p">,</span> <span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="nf">dec_layer</span><span class="p">(</span>
                <span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">slf_attn_mask</span><span class="o">=</span><span class="n">trg_mask</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="o">=</span><span class="n">src_mask</span><span class="p">)</span>
            <span class="n">dec_slf_attn_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">dec_slf_attn</span><span class="p">]</span> <span class="k">if</span> <span class="n">return_attns</span> <span class="k">else</span> <span class="p">[]</span>
            <span class="n">dec_enc_attn_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">dec_enc_attn</span><span class="p">]</span> <span class="k">if</span> <span class="n">return_attns</span> <span class="k">else</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">return_attns</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_slf_attn_list</span><span class="p">,</span> <span class="n">dec_enc_attn_list</span>
        <span class="k">return</span> <span class="n">dec_output</span><span class="p">,</span>
</code></pre></div></div>

<p><strong>整体Transformer</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">''' A sequence to sequence model with attention mechanism. '''</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="n">self</span><span class="p">,</span> <span class="n">n_src_vocab</span><span class="p">,</span> <span class="n">n_trg_vocab</span><span class="p">,</span> <span class="n">src_pad_idx</span><span class="p">,</span> <span class="n">trg_pad_idx</span><span class="p">,</span>
            <span class="n">d_word_vec</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_inner</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_position</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
            <span class="n">trg_emb_prj_weight_sharing</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">emb_src_trg_weight_sharing</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">scale_emb_or_prj</span><span class="o">=</span><span class="s">'prj'</span><span class="p">):</span>

        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">src_pad_idx</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">trg_pad_idx</span> <span class="o">=</span> <span class="n">src_pad_idx</span><span class="p">,</span> <span class="n">trg_pad_idx</span>

        <span class="c1"># In section 3.4 of paper "Attention Is All You Need", there is such detail:
</span>        <span class="c1"># "In our model, we share the same weight matrix between the two
</span>        <span class="c1"># embedding layers and the pre-softmax linear transformation...
</span>        <span class="c1"># In the embedding layers, we multiply those weights by \sqrt{d_model}".
</span>        <span class="c1">#
</span>        <span class="c1"># Options here:
</span>        <span class="c1">#   'emb': multiply \sqrt{d_model} to embedding output
</span>        <span class="c1">#   'prj': multiply (\sqrt{d_model} ^ -1) to linear projection output
</span>        <span class="c1">#   'none': no multiplication
</span>
        <span class="k">assert</span> <span class="n">scale_emb_or_prj</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'emb'</span><span class="p">,</span> <span class="s">'prj'</span><span class="p">,</span> <span class="s">'none'</span><span class="p">]</span>
        <span class="n">scale_emb</span> <span class="o">=</span> <span class="p">(</span><span class="n">scale_emb_or_prj</span> <span class="o">==</span> <span class="s">'emb'</span><span class="p">)</span> <span class="k">if</span> <span class="n">trg_emb_prj_weight_sharing</span> <span class="k">else</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale_prj</span> <span class="o">=</span> <span class="p">(</span><span class="n">scale_emb_or_prj</span> <span class="o">==</span> <span class="s">'prj'</span><span class="p">)</span> <span class="k">if</span> <span class="n">trg_emb_prj_weight_sharing</span> <span class="k">else</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">(</span>
            <span class="n">n_src_vocab</span><span class="o">=</span><span class="n">n_src_vocab</span><span class="p">,</span> <span class="n">n_position</span><span class="o">=</span><span class="n">n_position</span><span class="p">,</span>
            <span class="n">d_word_vec</span><span class="o">=</span><span class="n">d_word_vec</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="o">=</span><span class="n">d_inner</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="n">d_v</span><span class="p">,</span>
            <span class="n">pad_idx</span><span class="o">=</span><span class="n">src_pad_idx</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">scale_emb</span><span class="o">=</span><span class="n">scale_emb</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">Decoder</span><span class="p">(</span>
            <span class="n">n_trg_vocab</span><span class="o">=</span><span class="n">n_trg_vocab</span><span class="p">,</span> <span class="n">n_position</span><span class="o">=</span><span class="n">n_position</span><span class="p">,</span>
            <span class="n">d_word_vec</span><span class="o">=</span><span class="n">d_word_vec</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span><span class="o">=</span><span class="n">d_inner</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="n">d_k</span><span class="p">,</span> <span class="n">d_v</span><span class="o">=</span><span class="n">d_v</span><span class="p">,</span>
            <span class="n">pad_idx</span><span class="o">=</span><span class="n">trg_pad_idx</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">scale_emb</span><span class="o">=</span><span class="n">scale_emb</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">trg_word_prj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_trg_vocab</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> 

        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">==</span> <span class="n">d_word_vec</span><span class="p">,</span> \
        <span class="s">'To facilitate the residual connections, </span><span class="se">\
</span><span class="s">         the dimensions of all module outputs shall be the same.'</span>

        <span class="k">if</span> <span class="n">trg_emb_prj_weight_sharing</span><span class="p">:</span>
            <span class="c1"># Share the weight between target word embedding &amp; last dense layer
</span>            <span class="n">self</span><span class="p">.</span><span class="n">trg_word_prj</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">trg_word_emb</span><span class="p">.</span><span class="n">weight</span>

        <span class="k">if</span> <span class="n">emb_src_trg_weight_sharing</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">src_word_emb</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">trg_word_emb</span><span class="p">.</span><span class="n">weight</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_seq</span><span class="p">,</span> <span class="n">trg_seq</span><span class="p">):</span>

        <span class="n">src_mask</span> <span class="o">=</span> <span class="nf">get_pad_mask</span><span class="p">(</span><span class="n">src_seq</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">src_pad_idx</span><span class="p">)</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="nf">get_pad_mask</span><span class="p">(</span><span class="n">trg_seq</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">trg_pad_idx</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nf">get_subsequent_mask</span><span class="p">(</span><span class="n">trg_seq</span><span class="p">)</span>

        <span class="n">enc_output</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src_seq</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">dec_output</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">trg_seq</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">seq_logit</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">trg_word_prj</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">scale_prj</span><span class="p">:</span>
            <span class="n">seq_logit</span> <span class="o">*=</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="k">return</span> <span class="n">seq_logit</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_logit</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[you can also rcdsedirect to assets like pdf]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="https://jy-stdio.github.io/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>https://jy-stdio.github.io/blog/2022/giscus-comments</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://jy-stdio.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog" /><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://jy-stdio.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with redirect</title><link href="https://jy-stdio.github.io/blog/2022/redirect/" rel="alternate" type="text/html" title="a post with redirect" /><published>2022-02-01T17:39:00+00:00</published><updated>2022-02-01T17:39:00+00:00</updated><id>https://jy-stdio.github.io/blog/2022/redirect</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2022/redirect/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[you can also redirect to assets like pdf]]></summary></entry><entry><title type="html">a post with diagrams</title><link href="https://jy-stdio.github.io/blog/2021/diagrams/" rel="alternate" type="text/html" title="a post with diagrams" /><published>2021-07-04T17:39:00+00:00</published><updated>2021-07-04T17:39:00+00:00</updated><id>https://jy-stdio.github.io/blog/2021/diagrams</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2021/diagrams/"><![CDATA[<p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin.
Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p>

<p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine.
Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW.
For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p>

<h2 id="mermaid">Mermaid</h2>

<p>Install mermaid using <code class="language-plaintext highlighter-rouge">node.js</code> package manager <code class="language-plaintext highlighter-rouge">npm</code> by running the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npm <span class="nb">install</span> <span class="nt">-g</span> mermaid.cli
</code></pre></div></div>

<p>The diagram below was generated by the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div>

<div class="jekyll-diagrams diagrams mermaid">
  <svg id="mermaid-1683561245850" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1683561245850 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1683561245850 .node circle,#mermaid-1683561245850 .node ellipse,#mermaid-1683561245850 .node polygon,#mermaid-1683561245850 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1683561245850 .node.clickable{cursor:pointer}#mermaid-1683561245850 .arrowheadPath{fill:#333}#mermaid-1683561245850 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1683561245850 .edgeLabel{background-color:#e8e8e8}#mermaid-1683561245850 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1683561245850 .cluster text{fill:#333}#mermaid-1683561245850 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1683561245850 .actor{stroke:#ccf;fill:#ececff}#mermaid-1683561245850 text.actor{fill:#000;stroke:none}#mermaid-1683561245850 .actor-line{stroke:grey}#mermaid-1683561245850 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1683561245850 .messageLine0,#mermaid-1683561245850 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1683561245850 #arrowhead{fill:#333}#mermaid-1683561245850 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1683561245850 .messageText{fill:#333;stroke:none}#mermaid-1683561245850 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1683561245850 .labelText,#mermaid-1683561245850 .loopText{fill:#000;stroke:none}#mermaid-1683561245850 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1683561245850 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1683561245850 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1683561245850 .section{stroke:none;opacity:.2}#mermaid-1683561245850 .section0{fill:rgba(102,102,255,.49)}#mermaid-1683561245850 .section2{fill:#fff400}#mermaid-1683561245850 .section1,#mermaid-1683561245850 .section3{fill:#fff;opacity:.2}#mermaid-1683561245850 .sectionTitle0,#mermaid-1683561245850 .sectionTitle1,#mermaid-1683561245850 .sectionTitle2,#mermaid-1683561245850 .sectionTitle3{fill:#333}#mermaid-1683561245850 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1683561245850 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1683561245850 .grid path{stroke-width:0}#mermaid-1683561245850 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1683561245850 .task{stroke-width:2}#mermaid-1683561245850 .taskText{text-anchor:middle;font-size:11px}#mermaid-1683561245850 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1683561245850 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1683561245850 .taskText0,#mermaid-1683561245850 .taskText1,#mermaid-1683561245850 .taskText2,#mermaid-1683561245850 .taskText3{fill:#fff}#mermaid-1683561245850 .task0,#mermaid-1683561245850 .task1,#mermaid-1683561245850 .task2,#mermaid-1683561245850 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1683561245850 .taskTextOutside0,#mermaid-1683561245850 .taskTextOutside1,#mermaid-1683561245850 .taskTextOutside2,#mermaid-1683561245850 .taskTextOutside3{fill:#000}#mermaid-1683561245850 .active0,#mermaid-1683561245850 .active1,#mermaid-1683561245850 .active2,#mermaid-1683561245850 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1683561245850 .activeText0,#mermaid-1683561245850 .activeText1,#mermaid-1683561245850 .activeText2,#mermaid-1683561245850 .activeText3{fill:#000!important}#mermaid-1683561245850 .done0,#mermaid-1683561245850 .done1,#mermaid-1683561245850 .done2,#mermaid-1683561245850 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1683561245850 .doneText0,#mermaid-1683561245850 .doneText1,#mermaid-1683561245850 .doneText2,#mermaid-1683561245850 .doneText3{fill:#000!important}#mermaid-1683561245850 .crit0,#mermaid-1683561245850 .crit1,#mermaid-1683561245850 .crit2,#mermaid-1683561245850 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1683561245850 .activeCrit0,#mermaid-1683561245850 .activeCrit1,#mermaid-1683561245850 .activeCrit2,#mermaid-1683561245850 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1683561245850 .doneCrit0,#mermaid-1683561245850 .doneCrit1,#mermaid-1683561245850 .doneCrit2,#mermaid-1683561245850 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1683561245850 .activeCritText0,#mermaid-1683561245850 .activeCritText1,#mermaid-1683561245850 .activeCritText2,#mermaid-1683561245850 .activeCritText3,#mermaid-1683561245850 .doneCritText0,#mermaid-1683561245850 .doneCritText1,#mermaid-1683561245850 .doneCritText2,#mermaid-1683561245850 .doneCritText3{fill:#000!important}#mermaid-1683561245850 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1683561245850 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1683561245850 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1683561245850 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1683561245850 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1683561245850 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1683561245850 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1683561245850 #compositionEnd,#mermaid-1683561245850 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1683561245850 #aggregationEnd,#mermaid-1683561245850 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1683561245850 #dependencyEnd,#mermaid-1683561245850 #dependencyStart,#mermaid-1683561245850 #extensionEnd,#mermaid-1683561245850 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1683561245850 .branch-label,#mermaid-1683561245850 .commit-id,#mermaid-1683561245850 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1683561245850 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[an example of a blog post with diagrams]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://jy-stdio.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://jy-stdio.github.io/blog/2021/distill</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
  <span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
  <span class="n">df</span><span class="p">,</span>
  <span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span>
  <span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span>
  <span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span>
  <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
  <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
  <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="s">'assets/plotly/demo.html'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>Unordered list can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">a post with github metadata</title><link href="https://jy-stdio.github.io/blog/2020/github-metadata/" rel="alternate" type="text/html" title="a post with github metadata" /><published>2020-09-28T21:01:00+00:00</published><updated>2020-09-28T21:01:00+00:00</updated><id>https://jy-stdio.github.io/blog/2020/github-metadata</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2020/github-metadata/"><![CDATA[<p>A sample blog page that demonstrates the accessing of github meta data.</p>

<h2 id="what-does-github-metadata-do">What does Github-MetaData do?</h2>
<ul>
  <li>Propagates the site.github namespace with repository metadata</li>
  <li>Setting site variables :
    <ul>
      <li>site.title</li>
      <li>site.description</li>
      <li>site.url</li>
      <li>site.baseurl</li>
    </ul>
  </li>
  <li>Accessing the metadata - duh.</li>
  <li>Generating edittable links.</li>
</ul>

<h2 id="additional-reading">Additional Reading</h2>
<ul>
  <li>If you’re recieving incorrect/missing data, you may need to perform a Github API<a href="https://github.com/jekyll/github-metadata/blob/master/docs/authentication.md"> authentication</a>.</li>
  <li>Go through this <a href="https://jekyll.github.io/github-metadata/">README</a> for more details on the topic.</li>
  <li><a href="https://github.com/jekyll/github-metadata/blob/master/docs/site.github.md">This page</a> highlights all the feilds you can access with github-metadata.
<br /></li>
</ul>

<h2 id="example-metadata">Example MetaData</h2>
<ul>
  <li>Host Name :</li>
  <li>URL :</li>
  <li>BaseURL :</li>
  <li>Archived :</li>
  <li>Contributors :</li>
</ul>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[a quick run down on accessing github metadata.]]></summary></entry><entry><title type="html">a post with twitter</title><link href="https://jy-stdio.github.io/blog/2020/twitter/" rel="alternate" type="text/html" title="a post with twitter" /><published>2020-09-28T15:12:00+00:00</published><updated>2020-09-28T15:12:00+00:00</updated><id>https://jy-stdio.github.io/blog/2020/twitter</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2020/twitter/"><![CDATA[<p>A sample blog page that demonstrates the inclusion of Tweets/Timelines/etc.</p>

<h1 id="tweet">Tweet</h1>
<p>An example of displaying a tweet:</p>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<h1 id="timeline">Timeline</h1>
<p>An example of pulling from a timeline:</p>
<div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<h1 id="additional-details">Additional Details</h1>
<p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><category term="formatting" /><summary type="html"><![CDATA[an example of a blog post with twitter]]></summary></entry><entry><title type="html">a post with disqus comments</title><link href="https://jy-stdio.github.io/blog/2015/disqus-comments/" rel="alternate" type="text/html" title="a post with disqus comments" /><published>2015-10-20T15:59:00+00:00</published><updated>2015-10-20T15:59:00+00:00</updated><id>https://jy-stdio.github.io/blog/2015/disqus-comments</id><content type="html" xml:base="https://jy-stdio.github.io/blog/2015/disqus-comments/"><![CDATA[<p>This post shows how to add DISQUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with disqus comments]]></summary></entry></feed>